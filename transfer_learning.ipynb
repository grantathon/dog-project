{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Transfer Learning Analysis\n",
    "\n",
    "The following notebook will provide a basis for understanding the benefits of transfer learning for computer vision applications. The most popular image classification algorithms involve the use of convolutional neural networks (CNNs), which have been researched extensively. Using transfer learning allows us to leverage this extensive research by \"standing on the shoulders of giants\".\n",
    "\n",
    "This notebook is broken up into several sections, including:\n",
    "\n",
    "1. Get data\n",
    "2. Build neural networks\n",
    "3. Discover suitable solver and learning rate\n",
    "4. Train neural network\n",
    "    1. Random weights\n",
    "    2. Fine tune ImageNet layers\n",
    "    3. Freeze ImageNet layers\n",
    "5. Analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get data\n",
    "Import necessary packages and prepare training, validation, and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from sklearn.datasets import load_files       \n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "num_classes = len(dog_names)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to prepare the data so that it's compatible with Keras and TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6680/6680 [00:50<00:00, 131.45it/s]\n",
      "100%|██████████| 835/835 [00:05<00:00, 146.20it/s]\n",
      "100%|██████████| 836/836 [00:05<00:00, 162.05it/s]\n"
     ]
    }
   ],
   "source": [
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build neural networks\n",
    "We'll be evaluating three models using the popular CNN architecture created by Google called Inception V3. The first model will contain layers with randomized weights. The second will contain layers with ImageNet pre-weights that can be fine-tuned. The third will contain layers with ImageNet pre-weights that will be frozen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's build the neural network containing randomized weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_scratch_model():\n",
    "    base_scratch_model = InceptionV3(include_top=False, weights=None, input_shape=train_tensors.shape[1:])\n",
    "\n",
    "    # Extend the base model\n",
    "    x = base_scratch_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    scratch_predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    scratch_model = Model(inputs=base_scratch_model.input, outputs=scratch_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the neural network that uses ImageNet pre-weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_finetune_model():\n",
    "    base_finetune_model = InceptionV3(include_top=False, weights='imagenet',\n",
    "                                      input_shape=train_tensors.shape[1:])\n",
    "\n",
    "    # Extend the base model\n",
    "    x = base_finetune_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    finetune_predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs=base_finetune_model.input, outputs=finetune_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll build the neural network that uses frozen ImageNet pre-weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_frozen_model():\n",
    "    base_frozen_model = InceptionV3(include_top=False, weights='imagenet', input_shape=train_tensors.shape[1:])\n",
    "\n",
    "    # Extend the base model\n",
    "    x = base_frozen_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    frozen_predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    frozen_model = Model(inputs=base_frozen_model.input, outputs=frozen_predictions)\n",
    "\n",
    "    # Freeze the base model's layers\n",
    "    for layer in base_frozen_model.layers:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've built the neural networks we want to analyze later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discover suitable solver and learning rate\n",
    "We want to perform a quick analysis to determine what's a good solver and learning rate for this particular application. To be clear, this is by no means an exhaustive analysis and is extremely naive. However, it's better than nothing and provides us at least a basic viewpoint into an optimal model.\n",
    "\n",
    "In terms of solvers, we'll take a look at SGD, Nesterov, and Adam. In terms of learning rates, we'll take a look at 1e-3, 5e-4, and 1e-4. We'll evaluate these sets on the scratch model. Later on when we train the pre-weight models, we'll use two learning rates because when a model starts out with pre-weights, we want to usually use a lower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sgd with a learning rate of 0.000500\n",
      "Train on 48 samples, validate on 20 samples\n",
      "Epoch 1/2\n",
      "48/48 [==============================] - 31s - loss: 5.0051 - acc: 0.0000e+00 - val_loss: 5.1840 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "48/48 [==============================] - 27s - loss: 4.6460 - acc: 0.0208 - val_loss: 5.1333 - val_acc: 0.0500\n",
      "Training sgd with a learning rate of 0.001000\n",
      "Train on 48 samples, validate on 20 samples\n",
      "Epoch 1/2\n",
      "48/48 [==============================] - 35s - loss: 5.0058 - acc: 0.0417 - val_loss: 5.5274 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "48/48 [==============================] - 30s - loss: 4.3720 - acc: 0.1250 - val_loss: 5.5600 - val_acc: 0.0000e+00\n",
      "[('sgd', 0.0005, <keras.callbacks.History object at 0x7f5d57a4b748>), ('sgd', 0.001, <keras.callbacks.History object at 0x7f5d393ec7f0>)]\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "from copy import deepcopy\n",
    "\n",
    "# Number of epochs to train\n",
    "epochs = 2\n",
    "\n",
    "solvers = ['sgd']\n",
    "learning_rates = [5e-4, 1e-3]\n",
    "# solvers = ['sgd', 'nesterov', 'adam']\n",
    "# learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "def train(model, solver, lr):\n",
    "    if solver.lower() == 'sgd':\n",
    "        model.compile(optimizer=SGD(lr=lr, momentum=0.9), loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    elif solver.lower() == 'nesterov':\n",
    "        model.compile(optimizer=SGD(lr=lr, momentum=0.9, nesterov=True), loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    elif solver.lower() == 'adam':\n",
    "        model.compile(optimizer=Adam(lr=lr, momentum=0.9, beta_1=0.9, beta_2=0.999, epsilon=1e-8),\n",
    "                      loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    result = model.fit(train_tensors[:48], train_targets[:48], validation_data=(valid_tensors[:20], valid_targets[:20]),\n",
    "                       epochs=epochs, batch_size=4, verbose=1)\n",
    "#     return model.fit(train_tensors, train_targets, validation_data=(valid_tensors, valid_targets),\n",
    "#                      epochs=epochs, batch_size=64, verbose=1)\n",
    "\n",
    "    return solver, lr, result\n",
    "\n",
    "results = []\n",
    "\n",
    "# Train using sets\n",
    "for solver in solvers:\n",
    "    for lr in learning_rates:\n",
    "        print('Training %s with a learning rate of %f' % (solver, lr))\n",
    "        model = create_finetune_model()\n",
    "        results.append(train(model, solver, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to now determine which one of these solver and learning rate sets to use, so we'll see which setup produced the lowest validation loss. Again, this is by no means a thorough analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sgd', 0.001, <keras.callbacks.History object at 0x7f21ac857ac8>)]\n",
      "('sgd', 0.001, <keras.callbacks.History object at 0x7f21ac857ac8>)\n",
      "{'val_loss': [5.4644955635070804, 5.4795576095581051], 'val_acc': [0.0, 0.0], 'loss': [3.4664315978686013, 2.9534252683321633], 'acc': [0.45833333333333331, 0.52083333333333337]}\n"
     ]
    }
   ],
   "source": [
    "print(results)\n",
    "# print(results[0])\n",
    "# print(results[0][2].history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dog-project",
   "language": "python",
   "name": "dog-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
